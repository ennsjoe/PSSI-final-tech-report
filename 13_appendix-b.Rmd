
# APPENDIX B. Summary of DFO Science PSSI Data Projects {#app-data}

```{r appendix-c-setup, include=FALSE}
source(here::here("_common.R"))


knitr::opts_chunk$set(
  # Tables in appendix
  tab.cap.pre = "Table B-",
  tab.cap.sep = ": ",
  tab.lp      = "tabB:",
  tab.topcaption = TRUE,

  # Figures in appendix
  fig.cap.pre = "Figure B-",
  fig.cap.sep = ": ",
  fig.lp      = "figB:",     # optional but recommended for a clean namespace
  fig.topcaption = TRUE
)



# Helper function for including figures from data_tools_figures directory
include_data_figure <- function(filename, caption = "") {
  data_fig_dir <- here("figures", "data_tools_figures")
  fig_path <- file.path(data_fig_dir, filename)
  
  if (dir.exists(data_fig_dir) && file.exists(fig_path)) {
    knitr::include_graphics(fig_path)
  } else {
    warning("Figure not found: ", fig_path)
    cat("*[Figure not available: ", filename, "]*\n", sep = "")
  }
}
```

#### 1. Salmon Tracking, Escapement, Assessment and Management Platform

The Salmon Tracking, Escapement, Assessment and Management ([STREAM](https://pac-salmon.dfo-mpo.gc.ca:8443/bcsn/#/)) Platform is a new online resource that helps improve how salmon escapement information is collected, shared, and understood. [STREAM](https://pac-salmon.dfo-mpo.gc.ca:8443/bcsn/#/) brings together data, tools, and easy‑to‑use applications to support the conservation and management of Pacific salmon.

Fisheries and Oceans staff require timely, consistent access to salmon data across sectors. The STREAM platform enhances the timeliness of salmon escapement data, standardizes the spatial organization of salmon information, and improves reporting on salmon status. This foundational information supports stock assessment biologists, fisheries managers, species at risk biologists, and other decision-makers in delivering informed, data-driven management.

The platform has three main areas: entering and uploading data, learning about salmon populations, and exploring status reports (Figure 1). Through these features, users can view information about salmon populations across different regions and access data that support science, monitoring, and decision‑making.

As STREAM continues to grow, additional funding would allow us to respond to user feedback, develop more tools that improve data quality, and make more information available to the public.

```{r stream-figure, echo=FALSE, out.width=NULL, out.height=NULL, fig.cap="Screen capture of the Salmon Tracking Escapement Assessment and Management (STREAM) Platform with the table of contents visible."}

include_data_figure("Figure1.png")
```

#### 2. Salmon Space

[Salmon Space](https://egispi.ent.dfo-mpo.ca/apps/SalmonSpace-Espacesaumon/) is an interactive map that lets you explore salmon information across British Columbia and Yukon. You can search by location or by salmon species to find data such as salmon escapement and stock status (Figure 2).

There is strong public demand for access to salmon data to enhance transparency and build trust in departmental decision-making. Salmon Space provides up-to-date salmon counts and status information in an accessible format. By improving data transparency, this application strengthens trust and alignment between internal and external biologists, Indigenous partners, and the broader community of salmon stakeholders and enthusiasts.

The map includes several optional layers, including census sites, conservation units, stock management units, and designatable units, allowing users to view salmon information at different scales. Data is updated weekly from the Fisheries and Oceans regional database, so the public can access recent information with confidence.

Users can download both tabular and spatial data directly from the site, or customize and print map views to support reports, presentations, and research.

```{r salmon-space-figure, echo=FALSE, out.width=NULL, out.height=NULL, fig.cap="Screen capture of Salmon Space showing the Osoyoos-Skaha-Okanagan Sockeye conservation unit (highlighted) with associated information displayed in a pop-up."}
include_data_figure("Figure2.png")
```

#### 3. Specimen Tracking and Analysis Management Platform (STAMP)

The Specimen Tracking and Analysis Management Platform (STAMP) is an integration and tracking system that supports field biologists, laboratories, and analysts by linking information about individual organism specimens across field sampling, tissue handling, and laboratory analysis. STAMP does not collect new data; it connects existing data systems like CREST, KREST and FOS with results data systems like Otomanager and the newly developed Genetics database to ensure that specimen provenance, measurements, and lab results can be reliably tracked and integrated.

Salmon stock assessments and other biological analyses are currently limited by fragmented data systems, inconsistent identifiers, and manual workflows that make it difficult to track tissue samples and link lab results back to individual organisms. STAMP addresses these challenges by providing a standardized unique specimen identifier and shared tracking infrastructure, improving data quality, traceability, and interoperability across projects and systems.

STAMP consists of an integration layer that connects field data systems, laboratory workflows, and results databases, along with tools that support specimen tracking, status reporting, and data linkage. These components enable users to follow specimens from field collection through laboratory processing and to join lab results with organism-level and project-level information.

As STAMP evolves, future development will focus on expanding support to additional lab types, species, and data systems, strengthening data standards and interoperability, and improving tools for querying, reporting, and stewardship. This will enable more efficient operations, reduce manual effort, and support the integrated analyses required by DFO's modern mandate. 

```{r stamp-figure, echo=FALSE, out.width=NULL, out.height=NULL, fig.cap="Screen capture of STAMP."}
include_data_figure("Figure3.png")
```

#### 4. SILscanner: Stream Inspection Log Digitization Platform

SILscanner is a cloud-based digitization and data recovery platform focused on converting paper Stream Inspection Logs (SILs) into machine-readable data. Stream Inspection Logs or SILs are records of how many salmon of each species were observed and counted during a sampling event on a particular day. SILs remain a significant component of salmon monitoring and assessment operations, and SILscanner supports operational programs by transforming these paper records into accessible digital data.

A large volume of salmon monitoring data continues to be captured on paper field forms, which are later keypunched by DFO staff. This creates delays in data availability, limits data quality control, and requires substantial manual transcription effort. SILscanner addresses this challenge by providing a standardized optical character recognition and form-based data extraction pipeline that recovers structured data directly from Stream Inspection Logs, reducing manual data entry and the risk of transcription errors.

The SILscanner platform consists of a cloud-based optical character recognition (OCR) and form-processing pipeline, secure cloud storage for digitized documents, and a database for indexing extracted data. The system is optimized for form-based data capture from SILs, while also supporting the digitization of handwritten and typewritten archival documents and data extraction from text-based PDFs. Digitized records are indexed and made accessible to analytical and text-mining tools, enabling downstream analysis and integration with other salmon data systems.

SILscanner has been rolled out in the Fraser, West Coast Vancouver Island (WCVI), and North Coast (NC) areas and continuing development to scale to additional areas and form types. Future development will focus on expanding regional coverage, improving data extraction accuracy and validation workflows, and strengthening integration with downstream databases. These enhancements will accelerate data availability, improve data quality, and modernize a critical but currently paper-based component of salmon data capture.

```{r silscanner-figure, echo=FALSE, out.width=NULL, out.height=NULL, fig.cap="Screen capture of SILScanner."}
include_data_figure("Figure4.png")
```

#### 5. DocFlow: AI-Assisted Document Processing for Regulatory and Program Data

DocFlow is an AI-assisted document processing platform designed to transform large collections of unstructured PDF documents into searchable, analyzable, and decision-ready data. It supports programs such as Fish and Fish Habitat Protection by unlocking information embedded in regulatory, monitoring, and authorization documents that are currently difficult and time-consuming to use.

Many critical program datasets, including those stored in systems like Program Activity Tracking for Habitat (PATH), exist primarily as unstructured PDF files with limited metadata. Retrieving information such as project impacts, habitat footprints, mitigation measures, or species at risk often requires manual review of large numbers of documents, taking weeks and producing inconsistent results. DocFlow addresses this challenge by combining optical character recognition, artificial intelligence, and subject-matter-expert guidance to extract structured data from documents in a transparent and reproducible way.

The DocFlow platform consists of a document ingestion and OCR pipeline, AI-based information extraction tools, and an expert-guided interface that allows users to define, validate, and refine how key information is identified within documents. Extracted data are stored in structured formats and linked across related documents using graph-based representations, enabling efficient search, analysis, and structuring across large document collections.

Initially developed as a proof of concept for PATH authorization documents, DocFlow is being scaled to additional document types, programs, and regions, supporting integrative analyses such as cumulative effects assessment, regulatory performance reporting, and long-term data stewardship.

```{r docflow-figure, echo=FALSE, out.width=NULL, out.height=NULL, fig.cap="Screen capture of DocFlow."}

include_data_figure("Figure5.png")
```

#### 6. DFO Salmon Data Standards

The Government of Canada (GC) Fisheries and Oceans Canada (DFO) Salmon Ontology is a W3C-published semantic schema that provides a common, machine-readable framework for describing Pacific salmon data across DFO. It defines core concepts—such as stocks, surveys, samples, measurements, methods, and results—and their relationships using a hybrid Web Ontology Language (OWL) and Simple Knowledge Organization System (SKOS) approach. The ontology is schema-only by design, meaning it contains no data values, which supports clean versioning, reuse across systems, and interoperability with external platforms.

The ontology's purpose is to reduce friction in data integration, discovery, and reuse by providing scientists, data stewards, and managers with a shared, well-defined vocabulary. It aligns with international standards including Darwin Core, Basic Formal Ontology (BFO), Information Artifact Ontology (IAO), Sensor, Observation, Sample, and Actuator (SOSA), Provenance Ontology (PROV), and the InteroperAble Descriptions of Observable Property (I-ADOPT) framework. This alignment supports FAIR (Findable, Accessible, Interoperable, Reusable) and CARE principles while enabling future automation such as validation, provenance tracking, and semantic querying without locking DFO into a single system or tool.

#### 7. Salmon Population Summary Database (SPSR)

The Salmon Population Summary Database (SPSR) is a regional repository designed to centralize key salmon population index data—such as spawner abundance, catch, recruitment, exploitation rates, age structure, and hatchery contribution—that underpin routine stock assessments and forecasting. It addresses a major program gap where derived index data are currently scattered across local spreadsheets and bespoke systems, limiting transparency, efficiency, and trust. SPSR is intended to support Fisheries Science Advisory Reports (FSARs) and related assessment work by providing a consistent, FAIR-aligned source of core indices and associated metadata.

The database compiles standardized population indices and reference points at Conservation Unit (CU) and Stock Management Unit (SMU) scales, without replacing authoritative source systems such as NuSEDS, Fishery Operations System (FOS), or coded-wire tag (CWT) databases. It documents methods, assumptions, infilling approaches, and data quality to support reproducibility, succession, and training. SPSR is modeled on established repositories used by the National Oceanic and Atmospheric Administration (NOAA) and is intended for DFO science and assessment staff, with future development focused on expanding coverage, improving interoperability, and supporting strategic reviews of monitoring and assessment frameworks.

#### 8. Open Science Documentation Hub

The Data Stewardship Unit (DSU) is part of the Fishery and Assessment Data Section (FADS) within the Pacific Region Science Branch of Fisheries and Oceans Canada (DFO). The Open Science Documentation Hub provides a public, authoritative reference for data stewardship practices that support salmon science and assessment. It addresses common challenges related to inconsistent data management, limited documentation, and barriers to reproducible analysis across programs.

The hub supports biologists, analysts, and data stewards by documenting practical guidance, standards, and workflows across the full data lifecycle—from planning and collection to analysis and publication. Content is grounded in real biological and management contexts, including stock assessment and monitoring, and is aligned with FAIR data principles and Indigenous data governance considerations such as OCAP®. The site complements internal FADS resources by making core guidance openly accessible.

#### 9. FADS Salmon Data Wiki

The FADS Salmon Data Wiki is an internal, community-maintained reference for DFO Pacific Region Science staff that documents the salmon data landscape across programs, databases, and systems. Its purpose is to support orientation, onboarding, and day-to-day work by providing clear, factual descriptions of datasets, infrastructure, and contacts, without replacing authoritative databases or formal scientific documentation.

The wiki organizes information by major data domains (e.g., fisheries, escapement, genetics, enhancement, habitat) and describes how key systems are accessed and used in biological and management contexts such as stock assessment and fisheries management. It is available to all DFO staff and relies on ongoing contributions to remain current. Future work includes improving consistency across entries, strengthening links to authoritative metadata and open data records, and expanding coverage as systems evolve.

#### 10. Custom Salmon Data GPT

This supports salmon biologists and data stewards by converting messy spreadsheets into standardized Salmon Data Packages (SDPs) that follow a common specification and shared ontology. It streamlines biological and management data workflows by generating consistent metadata, controlled vocabularies, and semantically enriched variable definitions aligned with the DFO Salmon Ontology, Darwin Core, and I-ADOPT measurement patterns. This approach modernizes salmon data management by embedding ontology-based methods and reproducible standards directly into day-to-day data preparation, improving comparability across stock assessment, monitoring, genetics, and fisheries management datasets.

The project is accessed as a Custom GPT in ChatGPT and is intended for DFO scientists, data stewards, and collaborators, with outputs that are openly reusable as CSV-based SDPs. Next steps focus on expanding ontology coverage, refining measurement patterns, and further automating validation through companion tools (e.g., metasalmon). Simple visuals—such as SDP workflow diagrams or ontology relationship graphics—are often included to help users quickly understand how raw data are transformed into standardized, reusable packages.

#### 11. Salmon Data Package (SDP)

The Salmon Data Package (SDP) is a lightweight, frictionless-data-style specification for exchanging salmon datasets between scientists, assessment biologists, and data stewards. It is designed to work with familiar formats such as Excel and CSV while remaining ontology-aware, linking columns and code lists to the DFO Salmon Ontology and related vocabularies. SDP provides a consistent way to describe datasets, tables, variables, and categorical codes without enforcing a single rigid schema.

SDP enables automated validation, reproducible analysis, and interoperability across projects by standardizing metadata and semantics. It supports AI-assisted workflows, integration with R and Python tools (including the metasalmon package), and alignment with standards such as Frictionless Data and Darwin Core. The specification is openly available, versioned, and intended for use by DFO staff and collaborators preparing data for analysis, sharing, publication, or future knowledge-graph integration.

#### 12. Salmon Escapement Estimates Classification Toolkit

The Salmon Escapement Estimates Classification Toolkit is an interactive R Shiny application designed to standardize how salmon escapement estimate types are classified across DFO Stock Assessment groups. It responds to long-standing inconsistencies in how estimate types (Type 1–6) have been applied in NuSEDS, which can affect interpretation of data quality and downstream uses such as Wild Salmon Policy (WSP) rapid status assessments. The toolkit operationalizes updated guidance into a transparent, repeatable decision process.

The application guides users through a three-phase, dichotomous classification key that evaluates enumeration methods, estimation methods, and documentation and accuracy requirements. It provides traceable results, clear downgrade logic, and reference visuals to support consistent application across regions and programs. The tool is publicly accessible via ShinyApps.io for DFO staff and collaborators and is supported by structured YAML logic and reproducible documentation outputs. Future work includes refining guidance based on user feedback, aligning outputs more tightly with NuSEDS tables, and supporting broader integration with assessment workflows and training materials.

#### 13. Salmon Outlook Report Automation

The Salmon Outlook Report project modernizes the annual Salmon Outlook process, which produces categorical and numeric forecasts of salmon abundance by Stock Management Unit and Conservation Unit to support harvest planning. It addresses tight timelines and fragmented workflows by streamlining how Outlook data are collected, processed, and transformed into tables, reports, and presentations used by science and management.

The project uses Survey123 for structured data collection and R-based workflows to automate report and presentation generation. It supports semi-automated slide decks tailored to different audiences and produces technical reports using R Markdown and the csasdown format. The tools are intended for DFO science and assessment staff and are actively under development, with next steps focused on refining data inputs, stabilizing scripts, and improving integration with evolving crosswalks and Outlook processes.

#### 14. Genetics Results Database (GRD)

The Genetics Results Database (GRD) is a governed, centralized data product intended to serve as the authoritative source for Genetic Stock Identification (GSI) and Parentage-Based Tagging (PBT) results across DFO programs. It addresses fragmentation, inconsistent formats, and version conflicts that currently slow reuse and reduce confidence in genetics outputs. The GRD shifts long-term stewardship responsibility away from analytical labs toward a sustainable, FAIR-aligned data product with clear ownership and accountability.

The GRD supports science, assessment, and management by providing traceable, versioned genetics results with standardized metadata, aligned with FAIR and CARE principles and Indigenous data governance. Governance roles are clearly defined across the data lifecycle, with integration planned across regional and enterprise systems such as STAMP, CREST, and FOS. The database is intended for DFO staff, collaborators, and partners, with phased development beginning in 2025 and broader system integration in 2026. Success is measured by adoption, reduced integration effort, and sustained governance.

\newpage
